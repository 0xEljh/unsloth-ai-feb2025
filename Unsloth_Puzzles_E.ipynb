{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "<a name=\"MATH\"></a>\n",
    "## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n",
    "\n",
    "In LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n",
    "\n",
    "For example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n",
    "\n",
    "In Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n",
    "\n",
    "Our goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\n",
    "f \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n",
    "\n",
    "Notice during backpropagation we can use the chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n",
    "\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n",
    "\\frac{dy}{dX} &= W^T \\\\\n",
    "\\frac{dy}{dW} &= X^T \\\\\n",
    "\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n",
    "\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n",
    "\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n",
    "\n",
    "2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n",
    "\n",
    "3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n",
    "\n",
    "4. Make the Cross Entropy Loss work. You must show other functions working as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Criteria for E) Max points = 10\n",
    "```python\n",
    "if attemped_E:\n",
    "    E_score = 0\n",
    "    if VRAM_50_percent_reduction: E_score += 2\n",
    "    if remove_float32_upcast: E_score = 0\n",
    "    if show_ce_loss_works: E_score += 1\n",
    "    if show_other_functions_work: E_score += 1\n",
    "    if hardcoded_gradients: E_score = 0\n",
    "    if allows_dynamic_chunk_sizes: E_score += 1\n",
    "    if llama_1B_training_loss_matches: E_score += 1\n",
    "    else: E_score = 0\n",
    "    if GRPO_memory_efficient_linear_works: E_score += 4\n",
    "    final_score += E_score\n",
    "else:\n",
    "    final_score += 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = major_version >= 8\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "\n",
    "_F = lambda c: getframeinfo(c).lineno  # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\")  # Red colored warnings\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert x.dtype == dtype\n",
    "    try:\n",
    "        torch.testing.assert_close(x, y, check_stride=True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_function(batch, linear, labels):\n",
    "    x = linear(batch).float()  # Up projection to large space\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "\n",
    "    down_projection_function = CrossEntropyLoss(reduction=\"mean\")\n",
    "    # Down projection to small space\n",
    "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert device.type == \"cuda\", \"vram measurements won't work; check environment setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        \"\"\"\n",
    "        Naive forward pass that does NOT use chunking.\n",
    "        This computes the full projection XW at once.\n",
    "\n",
    "        Args:\n",
    "          - X: Input tensor (bs x seq_len x hidden) or (seq_len x hidden)\n",
    "          - linear: Linear projection function\n",
    "          - labels: Target labels (bs x seq_len) or (seq_len)\n",
    "          - forward_function: Function computing f(XW)\n",
    "\n",
    "        Returns:\n",
    "          - Computed loss (scalar) or transformed tensor\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute f(XW), e.g., cross-entropy loss\n",
    "            loss = forward_function(X, linear, labels)\n",
    "\n",
    "        # Save context for backward pass\n",
    "        # ctx.save_for_backward(X, labels)\n",
    "        ctx.save_for_backward(X, labels)\n",
    "        ctx.linear = linear\n",
    "        ctx.forward_function = forward_function\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dY):\n",
    "        \"\"\"\n",
    "        Naive backward pass that computes gradients from the saved full logits.\n",
    "\n",
    "        Args:\n",
    "          - dY: Gradient from the next layer (scalar or tensor)\n",
    "\n",
    "        Returns:\n",
    "          - Gradients w.r.t. X, None, None, None\n",
    "        \"\"\"\n",
    "\n",
    "        X, labels = ctx.saved_tensors\n",
    "        linear = ctx.linear\n",
    "        forward_function = ctx.forward_function\n",
    "\n",
    "        # Re-enable autograd for X and logits\n",
    "        with torch.enable_grad():\n",
    "            X.requires_grad = True\n",
    "            linear.weight.requires_grad = True\n",
    "\n",
    "            # Compute f(XW) again with autograd enabled to track gradients\n",
    "            loss = forward_function(X, linear, labels)\n",
    "\n",
    "        # Compute gradients with respect to X and W\n",
    "        loss.backward(dY)\n",
    "\n",
    "        return X.grad, None, None, None  # Grad w.r.t. X, W, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of a memory efficient linear requires the reduction method to be specified\n",
    "ie. `ctx.reduction` must be set to either mean or sum depending on the reduction method used in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, linear, labels, forward_function):\n",
    "        \"\"\"\n",
    "        Forward pass for memory-efficient linear projection + transformation.\n",
    "\n",
    "        - X: Input tensor, could be (bs x seq_len x hidden) or (seq_len x hidden)\n",
    "        - linear: Linear layer (performs projection to large space)\n",
    "        - labels: Target labels for cross-entropy loss ((bs) x seq_len x vocab)\n",
    "        - forward_function: Function that computes f(XW)\n",
    "        - chunk_size: How many rows to process at once\n",
    "\n",
    "        Returns:\n",
    "            The total loss computed in chunks\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        # EDIT THIS FUNCTION\n",
    "        # we want to compute f(XW) without ever triggering a compute of XW\n",
    "        # hence we do this in parts.\n",
    "\n",
    "        # we get >50 vram reduction at 4 chunks, can be changed.\n",
    "        num_chunks = 4\n",
    "        seq_dim = len(X.shape) - 2\n",
    "        seq_len = X.shape[seq_dim]\n",
    "        num_chunks = min(seq_len, num_chunks)\n",
    "        chunk_size = (seq_len + num_chunks - 1) // num_chunks\n",
    "\n",
    "        # we unfortunately also need to be aware of the reduction method.\n",
    "        # in the case of CE loss above, it's mean\n",
    "        ctx.reduction = \"mean\"\n",
    "\n",
    "        # save context for backward step\n",
    "        ctx.linear = linear\n",
    "        ctx.forward_function = forward_function\n",
    "        ctx.num_chunks = num_chunks\n",
    "        ctx.chunk_size = chunk_size\n",
    "        ctx.seq_dim = seq_dim\n",
    "        ctx.seq_len = seq_len\n",
    "        ctx.save_for_backward(X, labels)  # save chunked version instead?\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_chunks):\n",
    "                start = i * chunk_size\n",
    "                end = min((i + 1) * chunk_size, seq_len)\n",
    "\n",
    "                X_chunk = X.narrow(seq_dim, start, end - start).contiguous()\n",
    "                labels_chunk = labels.narrow(seq_dim, start, end - start).contiguous()\n",
    "\n",
    "                output_chunk = forward_function(X_chunk, linear, labels_chunk)\n",
    "                outputs.append(output_chunk)\n",
    "\n",
    "        # based on the type of output and reduction method involved, aggregate it:\n",
    "        # - scalar -> reduce to scalar\n",
    "        # - vector/tensors -> combine into a single vector/tensor by stacking rows\n",
    "\n",
    "        if isinstance(outputs[0], torch.Tensor) and outputs[0].dim() > 0:\n",
    "            # stack outputs\n",
    "            result = torch.cat(outputs, dim=seq_dim)  # validate choice of dimension\n",
    "        else:\n",
    "            # handle scalar case based on reduction method:\n",
    "            if ctx.reduction == \"sum\":\n",
    "                result = sum(outputs)\n",
    "            if ctx.reduction == \"mean\":\n",
    "                # handle weighting of outputs accordingly, based on chunk sizes\n",
    "                # some numel weighing?\n",
    "                # handle naively for now, assuming equal chunk sizing.\n",
    "                result = sum(outputs) / len(outputs)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dY):\n",
    "        # restore context:\n",
    "        X, labels = ctx.saved_tensors\n",
    "        linear = ctx.linear\n",
    "        forward_function = ctx.forward_function\n",
    "        num_chunks = ctx.num_chunks\n",
    "        chunk_size = ctx.chunk_size\n",
    "        seq_dim = ctx.seq_dim\n",
    "        seq_len = ctx.seq_len\n",
    "        reduction = ctx.reduction\n",
    "\n",
    "        dX = torch.zeros_like(X)  # Initialize gradient storage\n",
    "        # dW = torch.zeros_like(linear.weight)  # Store accumulated W gradient; not necessary\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        print(\n",
    "            f\"Peak VRAM, pre-compute: {torch.cuda.max_memory_allocated(device) / 1e6}\"\n",
    "        )\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start = i * chunk_size\n",
    "            end = min((i + 1) * chunk_size, seq_len)\n",
    "\n",
    "            X_chunk = X.narrow(seq_dim, start, end - start).contiguous()\n",
    "            labels_chunk = labels.narrow(seq_dim, start, end - start).contiguous()\n",
    "\n",
    "            # Recompute f(X_chunk W) to get the gradient\n",
    "            X_chunk.requires_grad = True\n",
    "\n",
    "            with torch.enable_grad():\n",
    "                output_chunk = forward_function(X_chunk, linear, labels_chunk)\n",
    "\n",
    "                # Compute gradients for chunk\n",
    "                if output_chunk.dim() == 0:  # scalar\n",
    "                    if reduction == \"mean\":\n",
    "                        output_chunk.backward(\n",
    "                            dY / num_chunks\n",
    "                        )  # rescale gradient based on reduction method\n",
    "                    if reduction == \"sum\":\n",
    "                        output_chunk.backward(dY)\n",
    "                else:\n",
    "                    # For non-scalar outputs (not our current case)\n",
    "                    # TODO: fix the chunking of dY; using the x_chunk shape is a bad reference\n",
    "                    # instead use output_chunk shape to guide this step\n",
    "                    output_chunk.backward(dY.narrow(seq_dim, start, end - start))\n",
    "\n",
    "                # Accumulate gradients\n",
    "                dX.narrow(seq_dim, start, end - start).copy_(\n",
    "                    X_chunk.grad * 2\n",
    "                )  # rescale gradients for dX\n",
    "\n",
    "                # do a cleanup\n",
    "                # since dW is additive, it shouldn't be contributing to peak ram.\n",
    "                # instead focus is on cleanups for dX\n",
    "                X_chunk.grad.detach_()\n",
    "                del X_chunk, labels_chunk, output_chunk\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        return dX, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_comparison(\n",
    "    batch_size=4,\n",
    "    seq_length=512,\n",
    "    hidden_dim=1024,\n",
    "    vocab_size=32_000,\n",
    "    loss_function=transformation_function,\n",
    "    seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare naive and memory-efficient implementations of linear projection.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size for test data\n",
    "        seq_length: Sequence length for test data\n",
    "        hidden_dim: Hidden dimension size\n",
    "        vocab_size: Vocabulary size for output dimension\n",
    "        loss_function: Function to transform linear output (default: transformation_function)\n",
    "        seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing comparison results\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type != \"cuda\":\n",
    "        print(\"Warning: VRAM tracking only works on CUDA. Running on CPU.\")\n",
    "    else:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Create test data\n",
    "    X = torch.randn(\n",
    "        batch_size,\n",
    "        seq_length,\n",
    "        hidden_dim,\n",
    "        device=device,\n",
    "        dtype=torch.float32,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    labels = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)\n",
    "\n",
    "    # Create identical linear layers\n",
    "    linear_naive = nn.Linear(hidden_dim, vocab_size, bias=False).to(device)\n",
    "    linear_efficient = nn.Linear(hidden_dim, vocab_size, bias=False).to(device)\n",
    "    linear_efficient.weight.data.copy_(linear_naive.weight.data)\n",
    "\n",
    "    # Helper function to run a test with given implementation\n",
    "    def run_test(implementation, X_input, linear_layer, base_memory=0):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "        # Forward and backward pass\n",
    "        loss = implementation.apply(X_input, linear_layer, labels, loss_function)\n",
    "        loss.backward()\n",
    "\n",
    "        # Collect results\n",
    "        results = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"X_grad\": X_input.grad.detach().clone(),\n",
    "            \"W_grad\": linear_layer.weight.grad.detach().clone(),\n",
    "        }\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            peak_memory = torch.cuda.max_memory_allocated(device)\n",
    "            results[\"peak_memory\"] = peak_memory\n",
    "            results[\"memory_used\"] = peak_memory - base_memory\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Run naive implementation\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    naive_base_memory = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "    naive_results = run_test(NaiveLinear, X, linear_naive, naive_base_memory)\n",
    "\n",
    "    # Zero gradients and prepare for efficient implementation\n",
    "    X.grad.zero_()\n",
    "    linear_naive.weight.grad.zero_()\n",
    "    X_eff = X.detach().clone().requires_grad_(True)\n",
    "\n",
    "    # Run memory-efficient implementation\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    efficient_base_memory = torch.cuda.max_memory_allocated(device)\n",
    "\n",
    "    efficient_results = run_test(\n",
    "        MemoryEfficientLinear, X_eff, linear_efficient, efficient_base_memory\n",
    "    )\n",
    "\n",
    "    # Compare results\n",
    "    comparisons = {\n",
    "        \"loss_match\": torch.allclose(\n",
    "            torch.tensor(naive_results[\"loss\"]),\n",
    "            torch.tensor(efficient_results[\"loss\"]),\n",
    "            rtol=1e-3,\n",
    "            atol=1e-5,\n",
    "        ),\n",
    "        \"X_grad_match\": torch.allclose(\n",
    "            naive_results[\"X_grad\"], efficient_results[\"X_grad\"], rtol=1e-3, atol=1e-5\n",
    "        ),\n",
    "        \"W_grad_match\": torch.allclose(\n",
    "            naive_results[\"W_grad\"], efficient_results[\"W_grad\"], rtol=1e-3, atol=1e-5\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Naive Loss: {naive_results['loss']:.6f}\")\n",
    "    print(f\"Memory-Efficient Loss: {efficient_results['loss']:.6f}\")\n",
    "    print(f\"Loss match: {comparisons['loss_match']}\")\n",
    "    print(f\"Gradient w.r.t X match: {comparisons['X_grad_match']}\")\n",
    "    print(f\"Gradient w.r.t W match: {comparisons['W_grad_match']}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"Naive Base VRAM Usage (MB): {naive_base_memory / 1e6:.2f}\")\n",
    "        print(f\"Naive Peak VRAM Usage (MB): {naive_results['peak_memory'] / 1e6:.2f}\")\n",
    "        print(f\"Efficient Base VRAM Usage (MB): {efficient_base_memory / 1e6:.2f}\")\n",
    "        print(\n",
    "            f\"Memory-Efficient Peak VRAM Usage (MB): {efficient_results['peak_memory'] / 1e6:.2f}\"\n",
    "        )\n",
    "\n",
    "        memory_saved_pct = (\n",
    "            1 - (efficient_results[\"memory_used\"] / naive_results[\"memory_used\"])\n",
    "        ) * 100\n",
    "        print(f\"Memory saved: {memory_saved_pct:.1f}%\")\n",
    "    else:\n",
    "        print(\"VRAM usage tracking not available on CPU.\")\n",
    "\n",
    "    return {\n",
    "        \"naive\": naive_results,\n",
    "        \"efficient\": efficient_results,\n",
    "        \"comparisons\": comparisons,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM, pre-compute: 585.38496\n",
      "Naive Loss: 10.554384\n",
      "Memory-Efficient Loss: 10.554384\n",
      "Loss match: True\n",
      "Gradient w.r.t X match: True\n",
      "Gradient w.r.t W match: True\n",
      "Naive Base VRAM Usage (MB): 272.65\n",
      "Naive Peak VRAM Usage (MB): 1076.12\n",
      "Efficient Base VRAM Usage (MB): 577.00\n",
      "Memory-Efficient Peak VRAM Usage (MB): 918.31\n",
      "Memory saved: 57.5%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'naive': {'loss': 10.554384231567383,\n",
       "  'X_grad': tensor([[[ 1.3807e-05, -1.6137e-05,  8.1403e-06,  ..., -1.3318e-05,\n",
       "            -1.3838e-05,  1.6917e-05],\n",
       "           [ 7.3444e-06, -8.2491e-06, -6.6505e-06,  ...,  9.7188e-06,\n",
       "             4.3857e-06,  2.1376e-05],\n",
       "           [ 2.0971e-05,  1.2372e-06,  2.5057e-05,  ..., -2.9860e-05,\n",
       "             1.3536e-05,  2.0116e-05],\n",
       "           ...,\n",
       "           [-2.0055e-05,  9.8789e-06, -2.3446e-05,  ...,  2.1487e-05,\n",
       "            -8.6920e-07,  1.6595e-05],\n",
       "           [ 1.1875e-05, -2.1308e-05,  5.3893e-06,  ..., -1.1348e-05,\n",
       "             2.1326e-05, -5.5247e-06],\n",
       "           [-4.9283e-06,  3.3811e-06, -2.7616e-05,  ..., -5.9093e-06,\n",
       "             2.8751e-05,  1.8444e-05]],\n",
       "  \n",
       "          [[-2.9359e-05, -2.4922e-05, -2.1881e-05,  ...,  2.0264e-05,\n",
       "            -2.0535e-05, -7.9485e-06],\n",
       "           [-1.0894e-05,  1.2523e-05, -2.0353e-05,  ...,  5.3034e-06,\n",
       "             2.0384e-05, -2.4650e-06],\n",
       "           [-2.0635e-05,  1.2297e-06,  1.5427e-05,  ..., -2.5713e-05,\n",
       "            -7.5527e-06,  2.4544e-05],\n",
       "           ...,\n",
       "           [ 1.3863e-05, -2.9394e-05, -2.0565e-05,  ...,  1.7361e-05,\n",
       "             8.8263e-06, -1.6468e-05],\n",
       "           [-1.3984e-06,  2.4721e-05, -2.1771e-05,  ...,  6.2345e-06,\n",
       "            -2.8369e-06, -1.4993e-05],\n",
       "           [-1.3915e-05,  1.3438e-05, -2.1925e-05,  ..., -2.6671e-05,\n",
       "             2.0439e-05, -1.8962e-05]],\n",
       "  \n",
       "          [[-1.8873e-05,  2.0428e-05, -1.4643e-05,  ..., -1.6266e-05,\n",
       "            -3.2093e-06,  4.9034e-06],\n",
       "           [ 2.8478e-05,  1.5941e-05,  1.6249e-05,  ..., -2.6839e-05,\n",
       "            -4.4021e-06, -2.1710e-05],\n",
       "           [ 3.6070e-06, -4.0264e-06,  1.9948e-05,  ...,  3.0446e-05,\n",
       "             9.3396e-06,  1.4031e-05],\n",
       "           ...,\n",
       "           [-8.2280e-06, -1.8130e-05,  8.2371e-06,  ...,  2.3859e-05,\n",
       "             1.7375e-05, -2.0455e-05],\n",
       "           [ 2.5795e-05,  8.9903e-06,  1.6647e-05,  ..., -2.8242e-05,\n",
       "             2.0198e-05,  2.8402e-05],\n",
       "           [ 3.2374e-06,  5.1836e-06, -6.0948e-06,  ..., -2.3209e-05,\n",
       "            -3.0265e-05, -1.0153e-05]],\n",
       "  \n",
       "          [[ 2.0347e-05, -3.0562e-05,  3.9499e-06,  ...,  2.1103e-05,\n",
       "             1.0928e-05,  2.9903e-06],\n",
       "           [ 1.7000e-05,  7.5356e-06, -1.5957e-05,  ..., -2.0246e-05,\n",
       "             2.5563e-05, -1.3079e-05],\n",
       "           [ 2.3559e-05,  5.0152e-06,  1.1245e-05,  ...,  4.1635e-06,\n",
       "            -2.2388e-05,  1.7437e-05],\n",
       "           ...,\n",
       "           [ 2.2010e-05,  2.0704e-05, -1.0172e-05,  ..., -1.1130e-05,\n",
       "             2.4579e-05,  1.0666e-05],\n",
       "           [-1.7235e-05,  1.8822e-05,  2.0956e-07,  ...,  1.2452e-05,\n",
       "            -2.8558e-05, -6.1498e-07],\n",
       "           [ 1.8298e-05,  3.3434e-06,  6.1584e-07,  ..., -1.0821e-05,\n",
       "             1.9977e-06,  1.0635e-05]]], device='cuda:0'),\n",
       "  'W_grad': tensor([[ 1.5648e-06,  9.8366e-07, -6.4117e-07,  ..., -1.1787e-06,\n",
       "           -2.3069e-06, -6.3991e-07],\n",
       "          [ 1.2116e-06,  1.9617e-07,  1.6747e-07,  ..., -5.1197e-07,\n",
       "           -1.4622e-06,  3.1488e-07],\n",
       "          [ 6.9320e-07,  6.5741e-07, -4.9840e-07,  ..., -2.3303e-07,\n",
       "           -2.0029e-06,  1.0158e-06],\n",
       "          ...,\n",
       "          [-1.1427e-06,  7.7551e-07, -1.1861e-06,  ..., -6.1624e-07,\n",
       "           -1.3135e-06,  1.7087e-08],\n",
       "          [ 7.5826e-07, -1.4260e-07, -8.3671e-07,  ..., -2.6718e-08,\n",
       "           -1.5793e-06, -4.2016e-07],\n",
       "          [ 1.6798e-06,  6.3127e-07, -1.2513e-06,  ...,  5.8090e-08,\n",
       "           -2.3523e-06,  7.2934e-07]], device='cuda:0'),\n",
       "  'peak_memory': 1076119040,\n",
       "  'memory_used': 803472896},\n",
       " 'efficient': {'loss': 10.554384231567383,\n",
       "  'X_grad': tensor([[[ 1.3807e-05, -1.6137e-05,  8.1403e-06,  ..., -1.3318e-05,\n",
       "            -1.3838e-05,  1.6917e-05],\n",
       "           [ 7.3444e-06, -8.2491e-06, -6.6505e-06,  ...,  9.7188e-06,\n",
       "             4.3857e-06,  2.1376e-05],\n",
       "           [ 2.0971e-05,  1.2372e-06,  2.5057e-05,  ..., -2.9860e-05,\n",
       "             1.3536e-05,  2.0116e-05],\n",
       "           ...,\n",
       "           [-2.0055e-05,  9.8789e-06, -2.3446e-05,  ...,  2.1487e-05,\n",
       "            -8.6920e-07,  1.6595e-05],\n",
       "           [ 1.1875e-05, -2.1308e-05,  5.3893e-06,  ..., -1.1348e-05,\n",
       "             2.1326e-05, -5.5247e-06],\n",
       "           [-4.9283e-06,  3.3811e-06, -2.7616e-05,  ..., -5.9093e-06,\n",
       "             2.8751e-05,  1.8444e-05]],\n",
       "  \n",
       "          [[-2.9359e-05, -2.4922e-05, -2.1881e-05,  ...,  2.0264e-05,\n",
       "            -2.0535e-05, -7.9485e-06],\n",
       "           [-1.0894e-05,  1.2523e-05, -2.0353e-05,  ...,  5.3034e-06,\n",
       "             2.0384e-05, -2.4650e-06],\n",
       "           [-2.0635e-05,  1.2297e-06,  1.5427e-05,  ..., -2.5713e-05,\n",
       "            -7.5527e-06,  2.4544e-05],\n",
       "           ...,\n",
       "           [ 1.3863e-05, -2.9394e-05, -2.0565e-05,  ...,  1.7361e-05,\n",
       "             8.8263e-06, -1.6468e-05],\n",
       "           [-1.3984e-06,  2.4721e-05, -2.1771e-05,  ...,  6.2345e-06,\n",
       "            -2.8369e-06, -1.4993e-05],\n",
       "           [-1.3915e-05,  1.3438e-05, -2.1925e-05,  ..., -2.6671e-05,\n",
       "             2.0439e-05, -1.8962e-05]],\n",
       "  \n",
       "          [[-1.8873e-05,  2.0428e-05, -1.4643e-05,  ..., -1.6266e-05,\n",
       "            -3.2093e-06,  4.9034e-06],\n",
       "           [ 2.8478e-05,  1.5941e-05,  1.6249e-05,  ..., -2.6839e-05,\n",
       "            -4.4021e-06, -2.1710e-05],\n",
       "           [ 3.6070e-06, -4.0264e-06,  1.9948e-05,  ...,  3.0446e-05,\n",
       "             9.3396e-06,  1.4031e-05],\n",
       "           ...,\n",
       "           [-8.2280e-06, -1.8130e-05,  8.2371e-06,  ...,  2.3859e-05,\n",
       "             1.7375e-05, -2.0455e-05],\n",
       "           [ 2.5795e-05,  8.9903e-06,  1.6647e-05,  ..., -2.8242e-05,\n",
       "             2.0198e-05,  2.8402e-05],\n",
       "           [ 3.2374e-06,  5.1836e-06, -6.0948e-06,  ..., -2.3209e-05,\n",
       "            -3.0265e-05, -1.0153e-05]],\n",
       "  \n",
       "          [[ 2.0347e-05, -3.0562e-05,  3.9499e-06,  ...,  2.1103e-05,\n",
       "             1.0928e-05,  2.9903e-06],\n",
       "           [ 1.7000e-05,  7.5356e-06, -1.5957e-05,  ..., -2.0246e-05,\n",
       "             2.5563e-05, -1.3079e-05],\n",
       "           [ 2.3559e-05,  5.0152e-06,  1.1245e-05,  ...,  4.1635e-06,\n",
       "            -2.2388e-05,  1.7437e-05],\n",
       "           ...,\n",
       "           [ 2.2010e-05,  2.0704e-05, -1.0172e-05,  ..., -1.1130e-05,\n",
       "             2.4579e-05,  1.0666e-05],\n",
       "           [-1.7235e-05,  1.8822e-05,  2.0956e-07,  ...,  1.2452e-05,\n",
       "            -2.8558e-05, -6.1498e-07],\n",
       "           [ 1.8298e-05,  3.3434e-06,  6.1584e-07,  ..., -1.0821e-05,\n",
       "             1.9977e-06,  1.0635e-05]]], device='cuda:0'),\n",
       "  'W_grad': tensor([[ 1.5648e-06,  9.8366e-07, -6.4117e-07,  ..., -1.1787e-06,\n",
       "           -2.3069e-06, -6.3991e-07],\n",
       "          [ 1.2116e-06,  1.9617e-07,  1.6747e-07,  ..., -5.1197e-07,\n",
       "           -1.4622e-06,  3.1488e-07],\n",
       "          [ 6.9320e-07,  6.5741e-07, -4.9840e-07,  ..., -2.3303e-07,\n",
       "           -2.0029e-06,  1.0158e-06],\n",
       "          ...,\n",
       "          [-1.1427e-06,  7.7551e-07, -1.1861e-06,  ..., -6.1624e-07,\n",
       "           -1.3135e-06,  1.7086e-08],\n",
       "          [ 7.5826e-07, -1.4260e-07, -8.3671e-07,  ..., -2.6717e-08,\n",
       "           -1.5793e-06, -4.2016e-07],\n",
       "          [ 1.6798e-06,  6.3127e-07, -1.2513e-06,  ...,  5.8091e-08,\n",
       "           -2.3523e-06,  7.2934e-07]], device='cuda:0'),\n",
       "  'peak_memory': 918312960,\n",
       "  'memory_used': 341317632},\n",
       " 'comparisons': {'loss_match': True,\n",
       "  'X_grad_match': True,\n",
       "  'W_grad_match': True}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comparison(\n",
    "    batch_size=4,\n",
    "    seq_length=512,\n",
    "    hidden_dim=1024,\n",
    "    vocab_size=32_000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM, pre-compute: 863.2576\n",
      "Naive Loss: 0.332687\n",
      "Memory-Efficient Loss: 0.332687\n",
      "Loss match: True\n",
      "Gradient w.r.t X match: True\n",
      "Gradient w.r.t W match: True\n",
      "Naive Base VRAM Usage (MB): 567.56\n",
      "Naive Peak VRAM Usage (MB): 1878.28\n",
      "Efficient Base VRAM Usage (MB): 854.87\n",
      "Memory-Efficient Peak VRAM Usage (MB): 1261.72\n",
      "Memory saved: 69.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'naive': {'loss': 0.3326873779296875,\n",
       "  'X_grad': tensor([[[ 9.1096e-08,  1.4354e-06, -1.5406e-07,  ...,  1.8944e-06,\n",
       "             5.3283e-07,  1.6396e-06],\n",
       "           [ 5.1661e-07, -1.4593e-06,  8.4060e-07,  ...,  8.0533e-07,\n",
       "            -1.8818e-06, -6.5520e-07],\n",
       "           [-3.1083e-07,  6.3797e-07,  1.2761e-07,  ..., -3.4181e-07,\n",
       "             5.3196e-07, -1.5012e-06],\n",
       "           ...,\n",
       "           [ 4.4087e-07, -1.1070e-06, -4.0084e-07,  ...,  1.8360e-07,\n",
       "             1.3163e-06, -1.0737e-06],\n",
       "           [-2.0768e-09,  6.2180e-07, -3.3828e-07,  ..., -4.3182e-07,\n",
       "            -2.8494e-07, -5.3675e-07],\n",
       "           [-1.7479e-07,  1.5999e-07,  5.8126e-07,  ..., -8.1810e-07,\n",
       "            -1.9800e-07,  4.7342e-07]],\n",
       "  \n",
       "          [[-3.0538e-07, -4.0315e-07, -1.8845e-07,  ..., -3.7491e-07,\n",
       "            -6.4160e-07,  8.8729e-07],\n",
       "           [-1.4186e-06,  8.8147e-08, -5.2613e-07,  ..., -5.2573e-07,\n",
       "            -1.0772e-06, -3.0598e-07],\n",
       "           [ 6.4900e-07,  1.3771e-07,  7.4652e-07,  ...,  2.5539e-07,\n",
       "             6.4766e-07,  1.5489e-07],\n",
       "           ...,\n",
       "           [ 1.8403e-07,  1.0987e-07,  1.2256e-06,  ..., -4.6236e-07,\n",
       "            -8.6097e-07,  4.0368e-07],\n",
       "           [-8.9381e-07, -9.6997e-07, -1.0647e-06,  ...,  9.0965e-07,\n",
       "             9.4734e-08,  1.5348e-08],\n",
       "           [-9.4111e-07,  2.1360e-07,  5.3846e-07,  ..., -1.6580e-07,\n",
       "            -1.4983e-06, -1.6250e-06]],\n",
       "  \n",
       "          [[ 1.4530e-06, -7.6111e-08,  2.0104e-07,  ..., -9.7767e-07,\n",
       "            -1.0043e-06,  3.8672e-07],\n",
       "           [ 5.9378e-07, -7.3326e-07, -4.8490e-07,  ...,  7.8846e-07,\n",
       "            -3.6524e-08, -1.6351e-07],\n",
       "           [-4.5692e-07, -3.0269e-07, -1.1027e-06,  ...,  3.8972e-07,\n",
       "             3.2805e-07, -4.6324e-08],\n",
       "           ...,\n",
       "           [ 5.8575e-07, -1.5098e-07,  4.6659e-08,  ..., -3.5372e-07,\n",
       "             1.7887e-07,  2.4639e-07],\n",
       "           [-1.4540e-07,  8.1190e-07,  1.0112e-06,  ...,  2.6770e-07,\n",
       "            -1.1352e-07, -5.3322e-07],\n",
       "           [-7.8030e-08,  1.5194e-07, -6.4372e-07,  ...,  6.7285e-07,\n",
       "            -1.3833e-06,  5.9359e-07]],\n",
       "  \n",
       "          [[ 3.3222e-07, -4.7915e-07,  8.5183e-07,  ...,  5.4683e-07,\n",
       "             1.8521e-07, -1.1306e-07],\n",
       "           [ 4.1908e-07, -6.1877e-07,  9.7518e-07,  ...,  7.7474e-07,\n",
       "            -4.6124e-07, -8.6573e-07],\n",
       "           [-6.0957e-07, -4.1383e-07,  8.5363e-07,  ...,  1.0740e-06,\n",
       "             2.8583e-07, -4.5336e-07],\n",
       "           ...,\n",
       "           [-7.5632e-08,  1.5573e-06, -3.7557e-07,  ..., -1.2861e-06,\n",
       "            -4.0763e-07,  5.8207e-07],\n",
       "           [ 1.1511e-07,  7.4082e-07, -5.0320e-07,  ..., -2.9013e-07,\n",
       "             5.7543e-07, -3.3018e-07],\n",
       "           [-4.8562e-07, -6.0521e-07,  1.6449e-07,  ..., -6.4810e-07,\n",
       "            -3.9703e-07,  5.0697e-07]]], device='cuda:0'),\n",
       "  'W_grad': tensor([[ 2.1267e-06,  1.4062e-06, -1.6860e-08,  ..., -1.6534e-07,\n",
       "           -1.0630e-06, -1.2095e-06],\n",
       "          [ 2.0691e-06, -2.6758e-07,  1.9031e-06,  ...,  1.1065e-06,\n",
       "           -3.6617e-07,  1.2606e-06],\n",
       "          [ 8.0769e-07,  1.1127e-06,  2.2139e-07,  ...,  1.8436e-06,\n",
       "           -6.7360e-07,  2.9128e-06],\n",
       "          ...,\n",
       "          [-2.0150e-06,  9.6174e-07, -1.3207e-06,  ...,  1.2948e-06,\n",
       "            7.1557e-08,  2.5986e-07],\n",
       "          [ 6.5956e-07, -6.2264e-07, -1.7717e-07,  ...,  2.0060e-06,\n",
       "            4.2129e-08, -5.4700e-07],\n",
       "          [ 2.3759e-06,  1.0144e-06, -1.7896e-06,  ...,  2.1291e-06,\n",
       "           -7.6880e-07,  1.9793e-06]], device='cuda:0'),\n",
       "  'peak_memory': 1878281216,\n",
       "  'memory_used': 1310723072},\n",
       " 'efficient': {'loss': 0.3326873779296875,\n",
       "  'X_grad': tensor([[[ 9.1096e-08,  1.4354e-06, -1.5406e-07,  ...,  1.8944e-06,\n",
       "             5.3283e-07,  1.6396e-06],\n",
       "           [ 5.1661e-07, -1.4593e-06,  8.4060e-07,  ...,  8.0533e-07,\n",
       "            -1.8818e-06, -6.5520e-07],\n",
       "           [-3.1083e-07,  6.3797e-07,  1.2761e-07,  ..., -3.4181e-07,\n",
       "             5.3196e-07, -1.5012e-06],\n",
       "           ...,\n",
       "           [ 4.4087e-07, -1.1070e-06, -4.0084e-07,  ...,  1.8360e-07,\n",
       "             1.3163e-06, -1.0737e-06],\n",
       "           [-2.0768e-09,  6.2180e-07, -3.3828e-07,  ..., -4.3182e-07,\n",
       "            -2.8494e-07, -5.3675e-07],\n",
       "           [-1.7479e-07,  1.5999e-07,  5.8126e-07,  ..., -8.1810e-07,\n",
       "            -1.9800e-07,  4.7342e-07]],\n",
       "  \n",
       "          [[-3.0538e-07, -4.0315e-07, -1.8845e-07,  ..., -3.7491e-07,\n",
       "            -6.4160e-07,  8.8729e-07],\n",
       "           [-1.4186e-06,  8.8147e-08, -5.2613e-07,  ..., -5.2573e-07,\n",
       "            -1.0772e-06, -3.0598e-07],\n",
       "           [ 6.4900e-07,  1.3771e-07,  7.4652e-07,  ...,  2.5539e-07,\n",
       "             6.4766e-07,  1.5489e-07],\n",
       "           ...,\n",
       "           [ 1.8403e-07,  1.0987e-07,  1.2256e-06,  ..., -4.6236e-07,\n",
       "            -8.6097e-07,  4.0368e-07],\n",
       "           [-8.9381e-07, -9.6997e-07, -1.0647e-06,  ...,  9.0965e-07,\n",
       "             9.4734e-08,  1.5348e-08],\n",
       "           [-9.4111e-07,  2.1360e-07,  5.3846e-07,  ..., -1.6580e-07,\n",
       "            -1.4983e-06, -1.6250e-06]],\n",
       "  \n",
       "          [[ 1.4530e-06, -7.6111e-08,  2.0104e-07,  ..., -9.7767e-07,\n",
       "            -1.0043e-06,  3.8672e-07],\n",
       "           [ 5.9378e-07, -7.3326e-07, -4.8490e-07,  ...,  7.8846e-07,\n",
       "            -3.6524e-08, -1.6351e-07],\n",
       "           [-4.5692e-07, -3.0269e-07, -1.1027e-06,  ...,  3.8972e-07,\n",
       "             3.2805e-07, -4.6324e-08],\n",
       "           ...,\n",
       "           [ 5.8575e-07, -1.5098e-07,  4.6659e-08,  ..., -3.5372e-07,\n",
       "             1.7887e-07,  2.4639e-07],\n",
       "           [-1.4540e-07,  8.1190e-07,  1.0112e-06,  ...,  2.6770e-07,\n",
       "            -1.1352e-07, -5.3322e-07],\n",
       "           [-7.8030e-08,  1.5194e-07, -6.4372e-07,  ...,  6.7285e-07,\n",
       "            -1.3833e-06,  5.9359e-07]],\n",
       "  \n",
       "          [[ 3.3222e-07, -4.7915e-07,  8.5183e-07,  ...,  5.4683e-07,\n",
       "             1.8521e-07, -1.1306e-07],\n",
       "           [ 4.1908e-07, -6.1877e-07,  9.7518e-07,  ...,  7.7474e-07,\n",
       "            -4.6124e-07, -8.6573e-07],\n",
       "           [-6.0957e-07, -4.1383e-07,  8.5363e-07,  ...,  1.0740e-06,\n",
       "             2.8583e-07, -4.5336e-07],\n",
       "           ...,\n",
       "           [-7.5632e-08,  1.5573e-06, -3.7557e-07,  ..., -1.2861e-06,\n",
       "            -4.0763e-07,  5.8207e-07],\n",
       "           [ 1.1511e-07,  7.4082e-07, -5.0320e-07,  ..., -2.9013e-07,\n",
       "             5.7543e-07, -3.3018e-07],\n",
       "           [-4.8562e-07, -6.0521e-07,  1.6449e-07,  ..., -6.4810e-07,\n",
       "            -3.9703e-07,  5.0697e-07]]], device='cuda:0'),\n",
       "  'W_grad': tensor([[ 2.1267e-06,  1.4062e-06, -1.6859e-08,  ..., -1.6533e-07,\n",
       "           -1.0630e-06, -1.2095e-06],\n",
       "          [ 2.0691e-06, -2.6758e-07,  1.9031e-06,  ...,  1.1065e-06,\n",
       "           -3.6617e-07,  1.2606e-06],\n",
       "          [ 8.0769e-07,  1.1127e-06,  2.2139e-07,  ...,  1.8436e-06,\n",
       "           -6.7360e-07,  2.9128e-06],\n",
       "          ...,\n",
       "          [-2.0150e-06,  9.6173e-07, -1.3207e-06,  ...,  1.2948e-06,\n",
       "            7.1558e-08,  2.5986e-07],\n",
       "          [ 6.5956e-07, -6.2264e-07, -1.7717e-07,  ...,  2.0060e-06,\n",
       "            4.2128e-08, -5.4700e-07],\n",
       "          [ 2.3759e-06,  1.0144e-06, -1.7896e-06,  ...,  2.1291e-06,\n",
       "           -7.6880e-07,  1.9793e-06]], device='cuda:0'),\n",
       "  'peak_memory': 1261721088,\n",
       "  'memory_used': 406853120},\n",
       " 'comparisons': {'loss_match': True,\n",
       "  'X_grad_match': True,\n",
       "  'W_grad_match': True}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternative loss function\n",
    "def mse_loss_function(batch, linear, labels):\n",
    "    \"\"\"\n",
    "    Alternative loss function using MSE loss instead of cross-entropy.\n",
    "    \"\"\"\n",
    "    x = linear(batch).float()\n",
    "    from torch.nn import MSELoss\n",
    "\n",
    "    # Create one-hot encoded targets for MSE loss\n",
    "    one_hot_labels = torch.zeros(\n",
    "        (labels.view(-1).size(0), linear.weight.size(0)), device=labels.device\n",
    "    )\n",
    "    one_hot_labels.scatter_(1, labels.view(-1, 1), 1)\n",
    "\n",
    "    # Apply MSE loss\n",
    "    loss_fn = MSELoss(reduction=\"mean\")\n",
    "    loss = loss_fn(x.view(-1, x.shape[-1]), one_hot_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "test_comparison(\n",
    "    batch_size=4,\n",
    "    seq_length=512,\n",
    "    hidden_dim=1024,\n",
    "    vocab_size=32_000,\n",
    "    loss_function=mse_loss_function,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
    "    \"expandable_segments:True,\" \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
    ")\n",
    "\n",
    "max_seq_length = 1024\n",
    "torch.set_default_dtype(torch.float16)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "dtype = torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Get LoRA and setup model\n",
    "model = get_peft_model(model, lora_config)\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if \".lora_A.\" in name or \".lora_B.\" in name:\n",
    "            param.requires_grad_(True)\n",
    "        else:\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "# Currently GC will cause torch.compile to be disabled, so disable it\n",
    "# model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Get dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": url}, split=\"train[:10%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.392800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.623400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.677300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.3818094968795775, metrics={'train_runtime': 7.9697, 'train_samples_per_second': 2.51, 'train_steps_per_second': 1.255, 'total_flos': 10592155496448.0, 'train_loss': 2.3818094968795775})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=1,\n",
    "        max_steps=10,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        seed=3407,\n",
    "        max_seq_length=max_seq_length,\n",
    "        fp16=model.get_input_embeddings().weight.dtype == torch.float16,\n",
    "        bf16=model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
    "        report_to=\"none\",  # For W&B\n",
    "        dataset_num_proc=4,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: patch the head with memory efficient linear\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
